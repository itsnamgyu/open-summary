The paper introduces BLIP-2, a Vision-Language pre-training (VLP) framework that employs a lightweight Querying Transformer (Q-Former) pre-trained in two stages to bridge the gap between vision and language, leveraging frozen pre-trained image encoders and large language models. The first stage entails vision-language representation learning from a frozen image encoder, while the second stage involves vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, surpassing existing methods with significantly fewer trainable parameters. It outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54 times fewer trainable parameters and showcases emerging capabilities for zero-shot image-to-text generation following natural language instructions based on visual knowledge reasoning, visual conversation, etc.

The proposed Querying Transformer serves as an information bottleneck, facilitating vision-language alignment with frozen unimodal models. BLIP-2 effectively leverages both frozen image models and language models and efficiently performs zero-shot image-to-text generation. It outperforms existing methods and illustrates potential for better VLP performance by leveraging more advanced unimodal models. BLIP-2 employs various pre-training objectives, including image-text contrastive learning, image-grounded text generation, and image-text matching, to facilitate effective vision-language alignment and improve performance on zero-shot VQA.

Furthermore, BLIP-2 achieves state-of-the-art performance on zero-shot image captioning and image-text retrieval, demonstrating strong generalization ability to out-domain images. It also surpasses existing methods on zero-shot visual question answering and image-text retrieval tasks. The paper provides detailed insights into the model architecture, pre-training settings, and experimental results, highlighting the effectiveness of the proposed framework. The results demonstrate that BLIP-2 is a generic and compute-efficient method for vision-language pre-training, presenting significant advancements in the field of multimodal conversational AI agents.
